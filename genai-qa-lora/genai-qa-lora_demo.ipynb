{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4de02f8c",
   "metadata": {},
   "source": [
    "# Domain-specific Q&A with Hugging Face and LoRA\n",
    "This notebook demonstrates a simple pipeline for fine-tuning a transformer-based model (e.g., LLaMA/GPT) on a domain-specific Q&A dataset using LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a02246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install -q transformers datasets peft accelerate bitsandbytes gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d386b1c",
   "metadata": {},
   "source": [
    "## Step 1: Load a Pretrained Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a902e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"tiiuae/falcon-7b-instruct\"  # Or any other model that supports LoRA\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f408a8",
   "metadata": {},
   "source": [
    "## Step 2: Apply LoRA with PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900abee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query_key_value\"],  # Adjust based on model architecture\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c601fa3",
   "metadata": {},
   "source": [
    "## Step 3: Load or Create a Simple Q&A Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11738d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Example: A few Q&A pairs for training\n",
    "data = {\n",
    "    \"question\": [\n",
    "        \"What documents are needed for an insurance claim?\",\n",
    "        \"How long is the cooling-off period?\",\n",
    "    ],\n",
    "    \"answer\": [\n",
    "        \"Typically, a completed claim form, policy number, and supporting invoices or reports.\",\n",
    "        \"Usually 14 days from the date of purchase or agreement.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data)\n",
    "dataset = dataset.map(lambda x: {\"text\": f\"Q: {x['question']} A: {x['answer']}\"})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fc354c",
   "metadata": {},
   "source": [
    "## Step 4: Train or Fine-tune the Model (Optional Demo Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bebeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skipping full training for demo â€“ insert Trainer setup here if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2c5765",
   "metadata": {},
   "source": [
    "## Step 5: Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4abafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=100)\n",
    "pipe(\"Q: What documents are needed for an insurance claim? A:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2679bfd9",
   "metadata": {},
   "source": [
    "## Step 6: Deploy with Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae7f88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def qa_bot(prompt):\n",
    "    result = pipe(f\"Q: {prompt} A:\")[0][\"generated_text\"]\n",
    "    return result.split(\"A:\")[-1].strip()\n",
    "\n",
    "gr.Interface(fn=qa_bot, inputs=\"text\", outputs=\"text\", title=\"Domain Q&A Bot\").launch()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
